Architectural considerations for big data workloads on OpenStack
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Abstract:**

Data intensive big data applications such as Kafka, Hadoop, and NoSQL are increasingly common in the modern data center, and now more than ever are being deployed at scale in OpenStack cloud environments. The performance demands placed on cloud infrastructure can be extreme, and in shared environment can often result in resource contention or outright stravation. This requires cloud infrastructure architected to support the diverse sets of requirements of these applications. In this talk we discuss considerations and approaches for handling the requirements of data intensive big data workloads in OpenStack such as: Defining big data workload applications proflies Architecting for high performance workloads Synthetic workload and full stack application testing


* **Christopher Power** *(Chris is a Principal Engineer at Comcast, where he is responsible for the architecture definition and implementation of scalable big data platforms and cloud infrastructure for Comcast's Elastic Cloud OpenStack environment. Most recently his work has been focused on architecting cloud infrastructure that is application aware by understanding the diverse demands that high performance systems such as Kafka, NoSQL, and Hadoop place on cloud infrastructure, and exploring different architectural and technical solutions to addressing these requirements. His previous work in this space involved deployment of Hadoop clusters at scale on OpenStack using Swift as the primary data store, including optimizations to the Hadoop-SwiftFS driver, and performance testing to evaluate various architectures. With over fifteen years of experience as a technology, product, and services leader in the telecommunications, mobile supply chain, consumer internet, and financial services industries, his previous roles include Director of Product Management where he was responsible for defining and executing product and technical strategy, Director of Professional Services where he was director and technical architect with P&L responsibility, numerous roles in software development, and a former member of the teaching staff for Harvard's CS50 Introduction to Computer Science course.)*

* **Andrew Leamon** *(Drew Leamon started his career at Microsoft while studying Computer Science at Princeton University.   In his studies, he delved into Computer Graphics, Artificial Intelligence and Computational Neurobiology.  At Microsoft, he collaborated with Microsoft Research on one of the first commercial implementations of collaborative filtering for e-commerce.  This was released as Microsoft Site Server: Commerce Edition.  Graduating into the DotCom boom, Drew caught the entrepreneurial spirit of the time and went on to sell cars on the internet through CarOrder.com, a Trilogy Software spin-off.   While there, he created new ways to sell content online through innovative configuration solutions.  Next Drew became one of the charter members of AirClic where he helped to create a platform to support workforce automation using wireless technologies.  Drew’s work and IP in this space became core to the company’s business value.   At Traffic.com / Navteq / Nokia, Drew pioneered the visualization of traffic data collected from highway sensors and digital probe devices.  Moving on to Comcast, Drew has taken his diverse experience and background and now leads part of the Engineering Analysis organization.  His team is developing advanced data visualizations for network data.  They are building elastically scaling Big Data infrastructure to support Analytic workloads.  Simulations of Comcast’s CDNs and platforms, developed by Drew’s team, are leveraging this platform and guiding the business and engineering teams.   His team is identifying high ROI opportunities.  They apply machine learning to datasets and are currently operationalizing the resulting predictive models to help improve customer experience.)*
