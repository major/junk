Achieving multi site VNF network readiness with less OPEX
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Abstract:**

Two of the primary cornerstones for NFV have been the reduction of OPEX, and the optimization of COTS resources. However, automating provisioning tasks across multiple, and possibly even heterogeneous, sites is proving to be quite challenging. For this reason, numerous use cases and requirements have been created around the matters. This presentation will show how to use the OpenStack Tricircle project to automate the network provisioning across multiple sites, as a precursor to very fast VNF networking via FD.io's VPP (Vector Packet Processing) project. 


* **Ash Young** *(I've been working in open source since 1993, primarily in RAID, file systems, iSCSI, and various other kernel/storage areas. I designed the first Linux and open source-based NAS appliance in 1997 at NetAttach, which was acquired by VA Linux Systems. I also developed the industry's first unified block/file stack back in 2003. I'm  a developer in OPNFV, FD.io, OCI, and in varios IoT communities.  I do also run an open source non-profit, Yunify, that is focused on creating software to help spark the NFV ecosystem. With the standards community deciding on OpenStack as the Virtual Infrastructure Manager (VIM) of choice, I am also very interested in building out the necessary infrastructure components to make this more optimal. This requires me to be fairly impartial as I look at all the various efforts, which might help bootstrap that ecosystem.  As the PTL for ONOSFW, I lead and develop a lightweight framework to instrument the NFVI (NFV Infrastructure). Currently, the first release of ONOSFW can be found in the Brahmaputra release of OPNFV and is still building out the network framework. The next release will add a unified Intent interface to the NFVI Networking.  Lastly, I am also one of only a few developers working in the area of NVMoE. NVMe (Non Volatile Memory express) is a storage protocol that was designed to remove the CPU as the storage bottleneck. However, due to the architectural constraints of PCIe, the only way to share NVMe storage, in a disaggregated fashion, places the bottleneck back in the CPU used to share access to the protocol. NVMoE places this technology onto an Ethernet fabric and allows the client to talk the NVMe protocol over Ethernet, in a fashion that eliminates the CPU bottleneck. I currently work on kernel, FTL, flash, drivers, and management software to help support this important ecosystem building block. )*
